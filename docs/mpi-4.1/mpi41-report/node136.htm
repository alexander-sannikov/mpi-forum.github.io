<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-coll/coll-rendered -->
<!-- with the command
tohtml -default -numbers -dosnl -htables -quietlatex -allgif -endpage mpi4-forum-tail.htm -Wnoredef --mpidoc --latexpgm pdflatex --indexfile mpi41-report-html.idx -basedef mpi4defs.txt -o mpi41-report.tex mpi-report.tex 
-->
<title>All-Reduce</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h2><span id="Node136">7.9.6. All-Reduce</span></h2>
<a href="node134.htm#Node135"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node129.htm#Node129"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node137.htm#Node137"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node129.htm#Node129"> Global Reduction Operations</a>
<b>Next: </b><a href="node137.htm#Node137"> <font face="sans-serif"> MPI</font> Process-Local Reduction</a>
<b>Previous: </b><a href="node134.htm#Node135"> Example of User-Defined Reduce</a>
<p>
  
  
<P> 
<font face="sans-serif"> MPI</font> includes   
a variant  
of the reduce operations  
where the result is returned to all <font face="sans-serif"> MPI</font> processes in  
a  
group.  
<font face="sans-serif"> MPI</font> requires that all <font face="sans-serif"> MPI</font> processes  
from the same group  
participating in these operations  
receive identical results.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_ALLREDUCE(<span style="white-space:nowrap">sendbuf</span>, <span style="white-space:nowrap">recvbuf</span>, <span style="white-space:nowrap">count</span>, <span style="white-space:nowrap">datatype</span>, <span style="white-space:nowrap">op</span>, <span style="white-space:nowrap">comm</span>)</TD></TR>  
<TR><TD> IN sendbuf</TD><TD>starting address of send buffer (choice)</TD></TR>  
<TR><TD> OUT recvbuf</TD><TD>starting address of receive buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in send buffer (non-negative integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of elements of send buffer (handle)</TD></TR>  
<TR><TD> IN op</TD><TD>operation (handle)</TD></TR>  
<TR><TD> IN comm</TD><TD>communicator (handle)</TD></TR>  
</TABLE>  
  <b> C binding</b><br>  <tt> int MPI_Allreduce(const void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm) <br></tt>  
  
  <tt> int MPI_Allreduce_c(const void *sendbuf, void *recvbuf, MPI_Count count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm) <br></tt>  
  <b> Fortran 2008 binding</b><br>  <tt> MPI_Allreduce(sendbuf, recvbuf, count, datatype, op, comm, ierror) <br><br>TYPE(*), DIMENSION(..), INTENT(IN) :: <span style="white-space:nowrap">sendbuf</span> <br>TYPE(*), DIMENSION(..) :: <span style="white-space:nowrap">recvbuf</span> <br>INTEGER, INTENT(IN) :: <span style="white-space:nowrap">count</span> <br>TYPE(MPI_Datatype), INTENT(IN) :: <span style="white-space:nowrap">datatype</span> <br>TYPE(MPI_Op), INTENT(IN) :: <span style="white-space:nowrap">op</span> <br>TYPE(MPI_Comm), INTENT(IN) :: <span style="white-space:nowrap">comm</span> <br>INTEGER, OPTIONAL, INTENT(OUT) :: <span style="white-space:nowrap">ierror</span> <br></tt>  
  <tt> MPI_Allreduce(sendbuf, recvbuf, count, datatype, op, comm, ierror) !(_c) <br><br>TYPE(*), DIMENSION(..), INTENT(IN) :: <span style="white-space:nowrap">sendbuf</span> <br>TYPE(*), DIMENSION(..) :: <span style="white-space:nowrap">recvbuf</span> <br>INTEGER(KIND=MPI_COUNT_KIND), INTENT(IN) :: <span style="white-space:nowrap">count</span> <br>TYPE(MPI_Datatype), INTENT(IN) :: <span style="white-space:nowrap">datatype</span> <br>TYPE(MPI_Op), INTENT(IN) :: <span style="white-space:nowrap">op</span> <br>TYPE(MPI_Comm), INTENT(IN) :: <span style="white-space:nowrap">comm</span> <br>INTEGER, OPTIONAL, INTENT(OUT) :: <span style="white-space:nowrap">ierror</span> <br></tt>  
  <b> Fortran binding</b><br>  <tt> MPI_ALLREDUCE(SENDBUF, RECVBUF, COUNT, DATATYPE, OP, COMM, IERROR) <br><br>&lt;type&gt; <span style="white-space:nowrap">SENDBUF(*)</span>, <span style="white-space:nowrap">RECVBUF(*)</span> <br>INTEGER <span style="white-space:nowrap">COUNT</span>, <span style="white-space:nowrap">DATATYPE</span>, <span style="white-space:nowrap">OP</span>, <span style="white-space:nowrap">COMM</span>, <span style="white-space:nowrap">IERROR</span> <br></tt>  
<P> 
If <font face="sans-serif"> comm</font> is an intra-communicator,  
<font face="sans-serif"> MPI_ALLREDUCE</font> behaves the  
same as <font face="sans-serif"> MPI_REDUCE</font> except that the result  
appears in the receive buffer of all the group members.  
<P> 
 
<br> 
<em> Advice  
        to implementors.</em>  
<P> 
The all-reduce operations can be implemented as a reduce, followed by a  
broadcast.  However, a direct implementation can lead to better performance.  
 (<em> End of advice to implementors.</em>) <br> 
The ``in place'' option  for intra-communicators is specified by passing the  
value <font face="sans-serif"> MPI_IN_PLACE</font> to the argument <font face="sans-serif"> sendbuf</font>  
at all processes.  
value <font face="sans-serif"> MPI_IN_PLACE</font> to the argument <font face="sans-serif"> sendbuf</font>  
at all <font face="sans-serif"> MPI</font> processes.  
In this case,  
the input data is taken at each <font face="sans-serif"> MPI</font> process from the receive buffer,  
where it will be replaced by the output data.  
<P> 
If <font face="sans-serif"> comm</font> is an inter-communicator, then the result of the reduction  
of the data provided by <font face="sans-serif"> MPI</font> processes in group A is stored at each <font face="sans-serif"> MPI</font> process  
in group B, and vice versa.    
Both groups should  
provide <font face="sans-serif"> count</font> and <font face="sans-serif"> datatype</font> arguments that specify the same type  
signature.  
<P> 
The following example uses an intra-communicator.  
<P> 
<br><b> Example</b>  
  
A routine that computes  
the product of a vector and an array that are distributed across a  
group of <font face="sans-serif"> MPI</font> processes and returns the answer at all nodes (see also  
Example <a href="node131.htm#Node131">Predefined Reduction Operations</a>).  
<P> 
<P><img width=744 height=524 src="img237.gif" alt="Image file"><P>
  
<P> 

<P>
<hr>
<a href="node134.htm#Node135"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node129.htm#Node129"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node137.htm#Node137"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node129.htm#Node129"> Global Reduction Operations</a>
<b>Next: </b><a href="node137.htm#Node137"> <font face="sans-serif"> MPI</font> Process-Local Reduction</a>
<b>Previous: </b><a href="node134.htm#Node135"> Example of User-Defined Reduce</a>
<p>
<HR>
Return to <A HREF="node601.htm">MPI-4.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-4.1 of November 2, 2023<BR>
HTML Generated on November 19, 2023
</FONT>
</body>
</html>
