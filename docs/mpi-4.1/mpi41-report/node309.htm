<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-one-side/one-side-2-rendered -->
<!-- with the command
tohtml -default -numbers -dosnl -htables -quietlatex -allgif -endpage mpi4-forum-tail.htm -Wnoredef --mpidoc --latexpgm pdflatex --indexfile mpi41-report-html.idx -basedef mpi4defs.txt -o mpi41-report.tex mpi-report.tex 
-->
<title>Window Creation</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h2><span id="Node309">13.2.1. Window Creation</span></h2>
<a href="node308.htm#Node308"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node308.htm#Node308"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node310.htm#Node310"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node308.htm#Node308"> Initialization</a>
<b>Next: </b><a href="node310.htm#Node310"> Window That Allocates Memory</a>
<b>Previous: </b><a href="node308.htm#Node308"> Initialization</a>
<p>
  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_WIN_CREATE(<span style="white-space:nowrap">base</span>, <span style="white-space:nowrap">size</span>, <span style="white-space:nowrap">disp_unit</span>, <span style="white-space:nowrap">info</span>, <span style="white-space:nowrap">comm</span>, <span style="white-space:nowrap">win</span>)</TD></TR>  
<TR><TD> IN base</TD><TD>initial address of window (choice)</TD></TR>  
<TR><TD> IN size</TD><TD>size of window in bytes (non-negative integer)</TD></TR>  
<TR><TD> IN disp_unit</TD><TD>local unit size for displacements, in bytes (positive integer)</TD></TR>  
<TR><TD> IN info</TD><TD>info argument (handle)</TD></TR>  
<TR><TD> IN comm</TD><TD>intra-communicator (handle)</TD></TR>  
<TR><TD> OUT win</TD><TD>window object (handle)</TD></TR>  
</TABLE>  
  <b> C binding</b><br>  <tt> int MPI_Win_create(void *base, MPI_Aint size, int disp_unit, MPI_Info info, MPI_Comm comm, MPI_Win *win) <br></tt>  
  
  <tt> int MPI_Win_create_c(void *base, MPI_Aint size, MPI_Aint disp_unit, MPI_Info info, MPI_Comm comm, MPI_Win *win) <br></tt>  
  <b> Fortran 2008 binding</b><br>  <tt> MPI_Win_create(base, size, disp_unit, info, comm, win, ierror) <br><br>TYPE(*), DIMENSION(..), ASYNCHRONOUS :: <span style="white-space:nowrap">base</span> <br>INTEGER(KIND=MPI_ADDRESS_KIND), INTENT(IN) :: <span style="white-space:nowrap">size</span> <br>INTEGER, INTENT(IN) :: <span style="white-space:nowrap">disp_unit</span> <br>TYPE(MPI_Info), INTENT(IN) :: <span style="white-space:nowrap">info</span> <br>TYPE(MPI_Comm), INTENT(IN) :: <span style="white-space:nowrap">comm</span> <br>TYPE(MPI_Win), INTENT(OUT) :: <span style="white-space:nowrap">win</span> <br>INTEGER, OPTIONAL, INTENT(OUT) :: <span style="white-space:nowrap">ierror</span> <br></tt>  
  <tt> MPI_Win_create(base, size, disp_unit, info, comm, win, ierror) !(_c) <br><br>TYPE(*), DIMENSION(..), ASYNCHRONOUS :: <span style="white-space:nowrap">base</span> <br>INTEGER(KIND=MPI_ADDRESS_KIND), INTENT(IN) :: <span style="white-space:nowrap">size</span>, <span style="white-space:nowrap">disp_unit</span> <br>TYPE(MPI_Info), INTENT(IN) :: <span style="white-space:nowrap">info</span> <br>TYPE(MPI_Comm), INTENT(IN) :: <span style="white-space:nowrap">comm</span> <br>TYPE(MPI_Win), INTENT(OUT) :: <span style="white-space:nowrap">win</span> <br>INTEGER, OPTIONAL, INTENT(OUT) :: <span style="white-space:nowrap">ierror</span> <br></tt>  
  <b> Fortran binding</b><br>  <tt> MPI_WIN_CREATE(BASE, SIZE, DISP_UNIT, INFO, COMM, WIN, IERROR) <br><br>&lt;type&gt; <span style="white-space:nowrap">BASE(*)</span> <br>INTEGER(KIND=MPI_ADDRESS_KIND) <span style="white-space:nowrap">SIZE</span> <br>INTEGER <span style="white-space:nowrap">DISP_UNIT</span>, <span style="white-space:nowrap">INFO</span>, <span style="white-space:nowrap">COMM</span>, <span style="white-space:nowrap">WIN</span>, <span style="white-space:nowrap">IERROR</span> <br></tt>  
<P> 
This procedure is collective over the group of  
<font face="sans-serif"> comm</font>.  It returns a handle to a window that can be used by the  
<font face="sans-serif"> MPI</font> processes in this group to perform <font face="sans-serif"> RMA</font> operations.  Each <font face="sans-serif"> MPI</font> process specifies  
a window of existing memory that it exposes to <font face="sans-serif"> RMA</font> accesses by any <font face="sans-serif"> MPI</font>  
processes in the group of  
<font face="sans-serif"> comm</font>.  
The window consists of <font face="sans-serif"> size</font> bytes,  
starting at address <font face="sans-serif"> base</font>.  
In C, <font face="sans-serif"> base</font> is the starting  
address of a memory region.   
In Fortran, one can pass the first element of a memory region  
or a whole array, which must be `simply contiguous'   
(for `simply contiguous,' see also   
Section <a href="node479.htm#Node479">Problems Due to Data Copying and Sequence Association with Subscript Triplets</a>).  
An <font face="sans-serif"> MPI</font> process may elect to expose no memory by  
specifying <font face="sans-serif"> size</font><font face="sans-serif">  = 0</font>.  
<P> 
The displacement unit argument  
is provided to facilitate address arithmetic in <font face="sans-serif"> RMA</font>  
operations: the target displacement argument of an <font face="sans-serif"> RMA</font> operation is  
scaled by the factor <font face="sans-serif"> disp_unit</font> specified by the target  
process, at window creation.  
<P> 
 
<br> 
<em> Rationale.</em>  
<P> 
The window size is specified using an address-sized integer,  
rather than a basic integer type, to allow windows that span more memory than  
can be described with a basic integer type.  
 (<em> End of rationale.</em>) <br> 
 
<br> 
<em> Advice to users.</em>  
<P> 
Common choices for <font face="sans-serif"> disp_unit</font>  
are 1 (no scaling), and (in C syntax) <tt>sizeof(type)</tt>, for a  
window that consists of an array of elements of type <tt>type</tt>.  The  
latter  
choice will allow one to use array indices in <font face="sans-serif"> RMA</font> calls,  
and have those scaled correctly to byte displacements, even in a  
heterogeneous environment.  
 (<em> End of advice to users.</em>) <br> 
The <font face="sans-serif"> info</font> argument provides  
optimization hints to the runtime about the expected  
usage pattern of the window.  
The following info keys are predefined:  
<P> 
<dl> 
 
<dt> 
<b><font face="sans-serif"> </font><tt>"</tt>no_locks<tt>"</tt></font> (boolean, default: <font face="sans-serif"> false</font>):</b><dd> 
if set to <font face="sans-serif"> true</font>,  
then the implementation may assume that passive target synchronization (i.e.,  
<font face="sans-serif"> MPI_WIN_LOCK</font>, <font face="sans-serif"> MPI_WIN_LOCK_ALL</font>) will not be used on  
the given window. This implies that this window is not used for 3-party  
communication, and <font face="sans-serif"> RMA</font> can be implemented with no (less) asynchronous  
agent activity at this process.  
 
<dt> 
<b><font face="sans-serif"> </font><tt>"</tt>accumulate_ordering<tt>"</tt></font> (string, default <font face="sans-serif"> rar,raw,war,waw</font>):</b><dd> 
controls the ordering of accumulate   
operations at the target.  See Section <a href="node339.htm#Node339">Ordering</a> for details.  
 
<dt> 
<b><font face="sans-serif"> </font><tt>"</tt>accumulate_ops<tt>"</tt></font> (string, default: <font face="sans-serif"> same_op_no_op</font>):</b><dd> 
if set to <font face="sans-serif"> </font><tt>"</tt>same_op<tt>"</tt></font>,  
the implementation will assume that all concurrent accumulate calls  
to the same target address will use the same operator. If set to  
<font face="sans-serif"> </font><tt>"</tt>same_op_no_op<tt>"</tt></font>, then the implementation will assume that  
all concurrent accumulate calls to the same target address will use the  
same operator or <font face="sans-serif"> MPI_NO_OP</font>. This can eliminate the need to  
protect access for certain operators where the hardware can  
guarantee atomicity.  
 
<dt> 
<b><font face="sans-serif"> </font><tt>"</tt>mpi_accumulate_granularity<tt>"</tt></font> (integer, default <tt>0</tt>):</b><dd> 
provides a hint to implementations about the desired synchronization granularity for accumulate operations, i.e., the size of memory ranges in bytes for which the implementation should acquire a synchronization primitive to ensure atomicity of updates.  
If the specified granularity is not divisible by the size of the type used in an accumulate operation, it should be treated as if it was the next multiple of the element size.  
For example, a granularity of <tt>1</tt> byte  should be treated as <tt>8</tt> in an accumulate operation using <font face="sans-serif"> MPI_UINT64_T</font>.  
By default, this info key is set to <tt>0</tt>, which leaves the choice of synchronization granularity to the implementation.  
If specified, all <font face="sans-serif"> MPI</font> processes in the group of a window must supply the same value.  
<P> 
  
 
<br> 
<em> Advice to users.</em>  
<P> 
Small synchronization granularities may provide improved latencies for accumulate operations with few elements and potentially increase concurrency of updates, at the cost of lower throughput.  
For example, a value matching the size of a type involved in an accumulate operation may enable implementations to use atomic memory operations instead of mutual exclusion devices.  
Larger synchronization granularities may yield higher throughput of accumulate operation with large numbers of elements due to lower synchronization costs, potentially at the expense of higher latency for accumulate operations with few elements, e.g., if atomic memory operations are not employed.  
By dividing larger accumulate operations into smaller segments, concurrent accumulate operations to the same window memory may update different segments in parallel.  
 (<em> End of advice to users.</em>) <br> 
 
<br> 
<em> Advice  
        to implementors.</em>  
<P> 
Implementations are encouraged to avoid mutual exclusion devices in cases where the granularity is small enough to warrant the use of atomic memory operations.  
For larger granularities, implementations should use this info value as a hint to partition the window memory into zones of mutual exclusion to enable segmentation of large accumulate operations.  
 (<em> End of advice to implementors.</em>) <br> 
 
<dt> 
<b><font face="sans-serif"> </font><tt>"</tt>same_size<tt>"</tt></font> (boolean, default: <font face="sans-serif"> false</font>):</b><dd> 
if set to <font face="sans-serif"> true</font>,  
then the implementation may assume that the argument <font face="sans-serif"> size</font> is  
identical on all <font face="sans-serif"> MPI</font> processes, and that all <font face="sans-serif"> MPI</font> processes have provided this  
info key with the same value.  
 
<dt> 
<b><font face="sans-serif"> </font><tt>"</tt>same_disp_unit<tt>"</tt></font> (boolean, default: <font face="sans-serif"> false</font>):</b><dd> 
if set to <font face="sans-serif"> true</font>,  
then the implementation  
may assume that the argument <font face="sans-serif"> disp_unit</font>  
is identical on all <font face="sans-serif"> MPI</font> processes, and  
that all <font face="sans-serif"> MPI</font> processes have provided this info key with the same value.  
 
<dt> 
<b></b><font face="sans-serif"> mpi_assert_memory_alloc_kinds</font> (string, not set by default):</b><dd> 
  
If set, the implementation may assume that the memory for all  
communication buffers passed to <font face="sans-serif"> MPI</font> operations performed by the calling <font face="sans-serif"> MPI</font>  
process on the given window will use only the memory allocation kinds  
listed in the value string.  
See Section <a href="node279.htm#Node279">Memory Allocation Info</a>.  
This info hint also applies to the window buffer provided in a call to  
<font face="sans-serif"> MPI_WIN_CREATE</font> or <font face="sans-serif"> MPI_WIN_ATTACH</font>. It does not  
apply to the memory  
allocated in a call to <font face="sans-serif"> MPI_WIN_ALLOCATE</font> or  
<font face="sans-serif"> MPI_WIN_ALLOCATE_SHARED</font>.  
</dl> 
<br> 
 
<br> 
<em> Advice to users.</em>  
<P> 
The info query mechanism described in Section <a href="node315.htm#Node315">Window Info</a>  
can be used to query the specified info arguments  
for  
windows that have been  
passed to a library. It is recommended that libraries check attached  
info keys for each passed window.  
 (<em> End of advice to users.</em>) <br> 
The various <font face="sans-serif"> MPI</font> processes in the group of  
<font face="sans-serif"> comm</font> may specify completely different  
target windows, in location, size, displacement  
units, and info arguments.   
As long as all the get, put and accumulate accesses   
to a particular <font face="sans-serif"> MPI</font> process fit their  
specific target window this should pose no problem.  
The same area in memory may appear in multiple windows, each  
associated with a different window object.  However, concurrent  
communications to distinct, overlapping windows may lead to  
undefined  
results.  
<P> 
Implementations may make the memory provided by the user available for load/store accesses  
by <font face="sans-serif"> MPI</font> processes in the same <em> shared memory domain</em>.  
A communicator of such processes can be constructed as described in Section <a href="node188.htm#Node188">Communicator Constructors</a> using <font face="sans-serif"> MPI_COMM_SPLIT_TYPE</font>.  
Pointers to access a <em> shared memory segment</em> can be queried  
using <font face="sans-serif"> MPI_WIN_SHARED_QUERY</font>.  
<P> 
 
<br> 
<em> Rationale.</em>  
<P> 
The reason for specifying the memory that may be accessed from another  
<font face="sans-serif"> MPI</font> process in an <font face="sans-serif"> RMA</font> operation is to permit the programmer to specify  
what memory can be a target of <font face="sans-serif"> RMA</font> operations and for the  
implementation to enforce that specification.  For example, with this  
definition, a server <font face="sans-serif"> MPI</font> process can safely allow a client <font face="sans-serif"> MPI</font> process to use  
<font face="sans-serif"> RMA</font> operations, knowing that (under the assumption that the <font face="sans-serif"> MPI</font>  
implementation does enforce the specified limits on the exposed  
memory) an error in the client cannot affect any memory other than  
what was explicitly exposed.    
 (<em> End of rationale.</em>) <br> 
 
<br> 
<em> Advice to users.</em>  
<P> 
A window can be created in any part of the <font face="sans-serif"> MPI</font> process memory.  However,  
on some systems, the performance of windows in  
memory allocated by <font face="sans-serif"> MPI_ALLOC_MEM</font>   
(Section <a href="node251.htm#Node251">Memory Allocation</a>) will be better.  
Also, on some systems, performance is improved when window boundaries  
are aligned at ``natural'' boundaries (word, double-word, cache line,  
page frame, etc.).  
 (<em> End of advice to users.</em>) <br> 
 
<br> 
<em> Advice  
        to implementors.</em>  
<P> 
In cases where <font face="sans-serif"> RMA</font> operations use different mechanisms in different  
memory areas (e.g., load/store accesses in a <em> shared memory segment</em>, and an  
asynchronous handler in private memory), the <font face="sans-serif"> MPI_WIN_CREATE</font>  
call needs to figure out which type of memory is used for the  
window.  To do so, <font face="sans-serif"> MPI</font> maintains, internally, the  
list of memory segments allocated by <font face="sans-serif"> MPI_ALLOC_MEM</font>, or by  
other, implementation-specific,  
mechanisms, together with information   
on the type of memory segment allocated.  When a call to   
<font face="sans-serif"> MPI_WIN_CREATE</font> occurs, then <font face="sans-serif"> MPI</font> checks which segment  
contains each window, and decides, accordingly, which mechanism to use  
for <font face="sans-serif"> RMA</font> operations.  
<P> 
Vendors may provide additional, implementation-specific mechanisms to  
allocate or to specify memory regions that are preferable for use in  
one-sided communication. In particular, such mechanisms can be used to  
place static variables into such preferred regions.  
<P> 
Implementors should document any performance impact of window alignment.  
 (<em> End of advice to implementors.</em>) <br> 

<P>
<hr>
<a href="node308.htm#Node308"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node308.htm#Node308"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node310.htm#Node310"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node308.htm#Node308"> Initialization</a>
<b>Next: </b><a href="node310.htm#Node310"> Window That Allocates Memory</a>
<b>Previous: </b><a href="node308.htm#Node308"> Initialization</a>
<p>
<HR>
Return to <A HREF="node601.htm">MPI-4.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-4.1 of November 2, 2023<BR>
HTML Generated on November 19, 2023
</FONT>
</body>
</html>
