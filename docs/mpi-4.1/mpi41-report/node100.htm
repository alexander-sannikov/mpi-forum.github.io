<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-datatypes/datatypes-rendered -->
<!-- with the command
tohtml -default -numbers -dosnl -htables -quietlatex -allgif -endpage mpi4-forum-tail.htm -Wnoredef --mpidoc --latexpgm pdflatex --indexfile mpi41-report-html.idx -basedef mpi4defs.txt -o mpi41-report.tex mpi-report.tex 
-->
<title>Distributed Array Datatype Constructor</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h2><span id="Node100">6.1.4. Distributed Array Datatype Constructor</span></h2>
<a href="node99.htm#Node99"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node96.htm#Node96"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node101.htm#Node101"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node96.htm#Node96"> Derived Datatypes</a>
<b>Next: </b><a href="node101.htm#Node101"> Address and Size Procedures</a>
<b>Previous: </b><a href="node99.htm#Node99"> Subarray Datatype Constructor</a>
<p>
  
<P> 
The distributed array type constructor supports  
HPF-like [<a href="node600.htm#-Bib49">49</a>] data distributions.  
However, unlike in HPF, the storage order may be specified for C arrays  
as well as for Fortran arrays.  
<P> 
 
<br> 
<em> Advice to users.</em>  
<P> 
One can create an HPF-like file view using this type constructor  
as follows.  
Complementary filetypes are created by having every process of a group  
call this constructor with identical arguments  
(with the exception of <font face="sans-serif"> rank</font> which should be set appropriately).  
These filetypes (along with identical <font face="sans-serif"> disp</font> and <font face="sans-serif"> etype</font>)  
are then used to define the view (via <font face="sans-serif"> MPI_FILE_SET_VIEW</font>),  
see <font face="sans-serif"> MPI</font> I/O, especially Section <a href="node350.htm#Node350">Definitions</a>  
and Section <a href="node361.htm#Node361">File Views</a>.  
Using this view,  
a collective data access operation (with identical offsets)  
will yield an HPF-like distribution pattern.  
 (<em> End of advice to users.</em>) <br> 
<TABLE><TR><TD COLSPAN=2>MPI_TYPE_CREATE_DARRAY(<span style="white-space:nowrap">size</span>, <span style="white-space:nowrap">rank</span>, <span style="white-space:nowrap">ndims</span>, <span style="white-space:nowrap">array_of_gsizes</span>, <span style="white-space:nowrap">array_of_distribs</span>, <span style="white-space:nowrap">array_of_dargs</span>, <span style="white-space:nowrap">array_of_psizes</span>, <span style="white-space:nowrap">order</span>, <span style="white-space:nowrap">oldtype</span>, <span style="white-space:nowrap">newtype</span>)</TD></TR>  
<TR><TD> IN size</TD><TD>size of process group (positive integer)</TD></TR>  
<TR><TD> IN rank</TD><TD>rank in process group (non-negative integer)</TD></TR>  
<TR><TD> IN ndims</TD><TD>number of array dimensions as well as process grid dimensions (positive integer)</TD></TR>  
<TR><TD> IN array_of_gsizes</TD><TD>number of elements of type <font face="sans-serif"> oldtype</font> in each dimension of global array (array of positive integers)</TD></TR>  
<TR><TD> IN array_of_distribs</TD><TD>distribution of array in each dimension (array of states)</TD></TR>  
<TR><TD> IN array_of_dargs</TD><TD>distribution argument in each dimension (array of positive integers)</TD></TR>  
<TR><TD> IN array_of_psizes</TD><TD>size of process grid in each dimension (array of positive integers)</TD></TR>  
<TR><TD> IN order</TD><TD>array storage order flag (state)</TD></TR>  
<TR><TD> IN oldtype</TD><TD>old datatype (handle)</TD></TR>  
<TR><TD> OUT newtype</TD><TD>new datatype (handle)</TD></TR>  
</TABLE>  
  <b> C binding</b><br>  <tt> int MPI_Type_create_darray(int size, int rank, int ndims, const int array_of_gsizes[], const int array_of_distribs[], const int array_of_dargs[], const int array_of_psizes[], int order, MPI_Datatype oldtype, MPI_Datatype *newtype) <br></tt>  
  
  <tt> int MPI_Type_create_darray_c(int size, int rank, int ndims, const MPI_Count array_of_gsizes[], const int array_of_distribs[], const int array_of_dargs[], const int array_of_psizes[], int order, MPI_Datatype oldtype, MPI_Datatype *newtype) <br></tt>  
  <b> Fortran 2008 binding</b><br>  <tt> MPI_Type_create_darray(size, rank, ndims, array_of_gsizes, array_of_distribs, array_of_dargs, array_of_psizes, order, oldtype, newtype, ierror) <br><br>INTEGER, INTENT(IN) :: <span style="white-space:nowrap">size</span>, <span style="white-space:nowrap">rank</span>, <span style="white-space:nowrap">ndims</span>, <span style="white-space:nowrap">array_of_gsizes(ndims)</span>, <span style="white-space:nowrap">array_of_distribs(ndims)</span>, <span style="white-space:nowrap">array_of_dargs(ndims)</span>, <span style="white-space:nowrap">array_of_psizes(ndims)</span>, <span style="white-space:nowrap">order</span> <br>TYPE(MPI_Datatype), INTENT(IN) :: <span style="white-space:nowrap">oldtype</span> <br>TYPE(MPI_Datatype), INTENT(OUT) :: <span style="white-space:nowrap">newtype</span> <br>INTEGER, OPTIONAL, INTENT(OUT) :: <span style="white-space:nowrap">ierror</span> <br></tt>  
  <tt> MPI_Type_create_darray(size, rank, ndims, array_of_gsizes, array_of_distribs, array_of_dargs, array_of_psizes, order, oldtype, newtype, ierror) !(_c) <br><br>INTEGER, INTENT(IN) :: <span style="white-space:nowrap">size</span>, <span style="white-space:nowrap">rank</span>, <span style="white-space:nowrap">ndims</span>, <span style="white-space:nowrap">array_of_distribs(ndims)</span>, <span style="white-space:nowrap">array_of_dargs(ndims)</span>, <span style="white-space:nowrap">array_of_psizes(ndims)</span>, <span style="white-space:nowrap">order</span> <br>INTEGER(KIND=MPI_COUNT_KIND), INTENT(IN) :: <span style="white-space:nowrap">array_of_gsizes(ndims)</span> <br>TYPE(MPI_Datatype), INTENT(IN) :: <span style="white-space:nowrap">oldtype</span> <br>TYPE(MPI_Datatype), INTENT(OUT) :: <span style="white-space:nowrap">newtype</span> <br>INTEGER, OPTIONAL, INTENT(OUT) :: <span style="white-space:nowrap">ierror</span> <br></tt>  
  <b> Fortran binding</b><br>  <tt> MPI_TYPE_CREATE_DARRAY(SIZE, RANK, NDIMS, ARRAY_OF_GSIZES, ARRAY_OF_DISTRIBS, ARRAY_OF_DARGS, ARRAY_OF_PSIZES, ORDER, OLDTYPE, NEWTYPE, IERROR) <br><br>INTEGER <span style="white-space:nowrap">SIZE</span>, <span style="white-space:nowrap">RANK</span>, <span style="white-space:nowrap">NDIMS</span>, <span style="white-space:nowrap">ARRAY_OF_GSIZES(*)</span>, <span style="white-space:nowrap">ARRAY_OF_DISTRIBS(*)</span>, <span style="white-space:nowrap">ARRAY_OF_DARGS(*)</span>, <span style="white-space:nowrap">ARRAY_OF_PSIZES(*)</span>, <span style="white-space:nowrap">ORDER</span>, <span style="white-space:nowrap">OLDTYPE</span>, <span style="white-space:nowrap">NEWTYPE</span>, <span style="white-space:nowrap">IERROR</span> <br></tt>  
<P> 
<font face="sans-serif"> MPI_TYPE_CREATE_DARRAY</font> can be used to generate  
the datatypes corresponding to the distribution  
of an <font face="sans-serif"> ndims</font>-dimensional array of <font face="sans-serif"> oldtype</font> elements  
onto  
an <font face="sans-serif"> ndims</font>-dimensional grid of logical processes.  
Unused dimensions of <font face="sans-serif"> array_of_psizes</font> should be set to <font face="sans-serif"> 1</font>  
(see Example <a href="node100.htm#Node100">Distributed Array Datatype Constructor</a>).  
For a call to <font face="sans-serif"> MPI_TYPE_CREATE_DARRAY</font> to be correct,  
the equation <i>prod<SUB>i=0</SUB><SUP>ndims-1</SUP> array_of_psizes[i] = size </i>  
must be satisfied.  
The ordering of processes in the process grid is assumed to be  
row-major, as in the case of virtual Cartesian process topologies.  
 
<br> 
<em> Advice to users.</em>  
<P> 
For both Fortran and C arrays, the ordering of processes in the  
process grid is assumed to be row-major. This is consistent with the  
ordering used in virtual Cartesian process topologies in  
<font face="sans-serif"> MPI</font>.  
To create such virtual process topologies, or to find the coordinates  
of a process in the process grid, etc., users may use the corresponding  
process topology procedures,  
see Chapter <a href="node218.htm#Node218">Virtual Topologies for <font face="sans-serif"> MPI</font> Processes</a>.  
 (<em> End of advice to users.</em>) <br> 
Each dimension of the array  
can be  
distributed in one of three ways:  
<P><img width=617 height=72 src="img121.gif" alt="Image file"><P>
The constant <font face="sans-serif"> MPI_DISTRIBUTE_DFLT_DARG</font> specifies  
a default distribution argument.  
The distribution argument for a dimension that is not distributed  
is ignored.  
For any dimension  
<font face="sans-serif"> i</font>  
in which the distribution  
is <font face="sans-serif"> MPI_DISTRIBUTE_BLOCK</font>,  
it is erroneous  
to specify  
<font face="sans-serif"> array_of_dargs[i]</font> <i>*</i> <font face="sans-serif"> array_of_psizes[i]</font>  
<i>&lt;</i> <font face="sans-serif"> array_of_gsizes[i]</font>.  
<P> 
For example, the HPF layout <tt>ARRAY(CYCLIC(15))</tt>  
corresponds to <font face="sans-serif"> MPI_DISTRIBUTE_CYCLIC</font>  
with a distribution argument of 15, and the HPF layout <tt>ARRAY(BLOCK)</tt>  
corresponds to  
<font face="sans-serif"> MPI_DISTRIBUTE_BLOCK</font> with a distribution argument of  
<font face="sans-serif"> MPI_DISTRIBUTE_DFLT_DARG</font>.  
<P> 
The <font face="sans-serif"> order</font> argument is used as in <font face="sans-serif"> MPI_TYPE_CREATE_SUBARRAY</font> to  
specify the storage order.  
Therefore, arrays described by this type constructor may be  
stored in Fortran (column-major) or C (row-major) order.  
Valid values for <font face="sans-serif"> order</font> are  
<font face="sans-serif"> MPI_ORDER_FORTRAN</font> and <font face="sans-serif"> MPI_ORDER_C</font>.  
<P> 
This routine creates a new <font face="sans-serif"> MPI</font> datatype with a typemap defined in  
terms of a function called ``cyclic()'' (see below).  
<P> 
Without loss of generality, it suffices to define the typemap  
for the <font face="sans-serif"> MPI_DISTRIBUTE_CYCLIC</font> case where  
<font face="sans-serif"> MPI_DISTRIBUTE_DFLT_DARG</font> is not used.  
<P> 
<font face="sans-serif"> MPI_DISTRIBUTE_BLOCK</font> and <font face="sans-serif"> MPI_DISTRIBUTE_NONE</font>  
can be reduced to the <font face="sans-serif"> MPI_DISTRIBUTE_CYCLIC</font> case  
for dimension  
<font face="sans-serif"> i</font>  
as follows.  
<P> 
<font face="sans-serif"> MPI_DISTRIBUTE_BLOCK</font> with  
<font face="sans-serif"> array_of_dargs[i]</font> equal to <font face="sans-serif"> MPI_DISTRIBUTE_DFLT_DARG</font>  
is equivalent to  
<font face="sans-serif"> MPI_DISTRIBUTE_CYCLIC</font>  
with <font face="sans-serif"> array_of_dargs[i]</font> set to  
<p><i> 
(mpiargarray_of_gsizes[i] + mpiargarray_of_psizes[i] - 1) 
        / mpiargarray_of_psizes[i]. 
</i><p>  
If <font face="sans-serif"> array_of_dargs[i]</font> is not <font face="sans-serif"> MPI_DISTRIBUTE_DFLT_DARG</font>,  
then <font face="sans-serif"> MPI_DISTRIBUTE_BLOCK</font> and <font face="sans-serif"> MPI_DISTRIBUTE_CYCLIC</font>  
are equivalent.  
<P> 
<font face="sans-serif"> MPI_DISTRIBUTE_NONE</font> is equivalent to  
<font face="sans-serif"> MPI_DISTRIBUTE_CYCLIC</font>  
with <font face="sans-serif"> array_of_dargs[i]</font> set to <font face="sans-serif"> array_of_gsizes[i]</font>.  
<P> 
Finally,  
<font face="sans-serif"> MPI_DISTRIBUTE_CYCLIC</font> with  
<font face="sans-serif"> array_of_dargs[i]</font> equal to <font face="sans-serif"> MPI_DISTRIBUTE_DFLT_DARG</font>  
is equivalent to  
<font face="sans-serif"> MPI_DISTRIBUTE_CYCLIC</font> with  
<font face="sans-serif"> array_of_dargs[i]</font> set to 1.  
<P> 
For <font face="sans-serif"> MPI_ORDER_FORTRAN</font>,  
an <font face="sans-serif"> ndims</font>-dimensional distributed array (<font face="sans-serif"> newtype</font>)  
is defined by the following code fragment:  
<P> 
<P><img width=729 height=225 src="img122.gif" alt="Image file"><P>
For <font face="sans-serif"> MPI_ORDER_C</font>, the code is:  
<P> 
<P><img width=729 height=225 src="img123.gif" alt="Image file"><P>
where <tt>r[i]</tt> is the position of the process (with rank <font face="sans-serif"> rank</font>)  
in the process grid at dimension <tt>i</tt>.  
The values of <tt>r[i]</tt> are given by the following code fragment:  
<P> 
<P><img width=729 height=225 src="img124.gif" alt="Image file"><P>
 Let the typemap of <font face="sans-serif"> oldtype</font> have the form:  
<img width=508 height=23 src="img125.gif" alt="Image file">
  
where <img width=42 height=19 src="img126.gif" alt="Image file">
 is a predefined <font face="sans-serif"> MPI</font> datatype, and let <i>ex</i> be the  
extent of <font face="sans-serif"> oldtype</font>.  
The following function uses the conceptual datatypes <font face="sans-serif"> lb_marker</font>  
and <font face="sans-serif"> ub_marker</font>, see Section <a href="node102.htm#Node102">Lower-Bound and Upper-Bound Markers</a> for details.  
<P> 
Given the above, the function cyclic() is defined as follows:  
<P><img width=810 height=1000 src="img127.gif" alt="Image file"><P>
where <i>count</i> is defined by this code fragment:  
<P><img width=729 height=125 src="img128.gif" alt="Image file"><P>
Here, <tt>nblocks</tt> is the number of blocks that must be  
distributed among the processors.  
Finally, <img width=67 height=21 src="img129.gif" alt="Image file">
 is defined by this code fragment:  
<P><img width=729 height=225 src="img130.gif" alt="Image file"><P>
<br><b> Example</b>  
Consider generating the filetypes corresponding to the HPF distribution:  
  
<P><img width=781 height=75 src="img131.gif" alt="Image file"><P>
This can be achieved by the following Fortran code,  
assuming there will be six processes attached to the run:  
<P><img width=808 height=449 src="img132.gif" alt="Image file"><P>
  
<P> 

<P>
<hr>
<a href="node99.htm#Node99"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node96.htm#Node96"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node101.htm#Node101"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node96.htm#Node96"> Derived Datatypes</a>
<b>Next: </b><a href="node101.htm#Node101"> Address and Size Procedures</a>
<b>Previous: </b><a href="node99.htm#Node99"> Subarray Datatype Constructor</a>
<p>
<HR>
Return to <A HREF="node601.htm">MPI-4.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-4.1 of November 2, 2023<BR>
HTML Generated on November 19, 2023
</FONT>
</body>
</html>
