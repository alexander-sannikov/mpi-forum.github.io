<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-part/part-rendered -->
<!-- with the command
tohtml -default -numbers -dosnl -htables -quietlatex -allgif -endpage mpi4-forum-tail.htm -Wnoredef --mpidoc --latexpgm pdflatex --indexfile mpi41-report-html.idx -basedef mpi4defs.txt -o mpi41-report.tex mpi-report.tex 
-->
<title>Semantics of Partitioned Point-to-Point Communication</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h1><span id="Node87">5.2. Semantics of Partitioned Point-to-Point Communication</span></h1>
<a href="node86.htm#Node86"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node85.htm#Node85"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node88.htm#Node88"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node85.htm#Node85"> Partitioned Point-to-Point Communication</a>
<b>Next: </b><a href="node88.htm#Node88"> Communication Initialization and Starting with Partitioning</a>
<b>Previous: </b><a href="node86.htm#Node86"> Introduction</a>
<p>
  
   
<font face="sans-serif"> MPI</font> guarantees certain general properties of  
partitioned point-to-point communication progress,  
which are described in this section.  
<P> 
Persistent communications use opaque <font face="sans-serif"> MPI_REQUEST</font> objects  
as described in Section <a href="node53.htm#Node53">Point-to-Point Communication</a>. Partitioned communication  
uses these same semantics for <font face="sans-serif"> MPI_REQUEST</font> objects.  
<P> 
Partitioned communication provides fine-grained transfers on either or  
both sides of a send-receive operation described by requests.  
Persistent communication semantics are ideal for partitioned  
communication: they provide <font face="sans-serif"> MPI_PSEND_INIT</font> and  
<font face="sans-serif"> MPI_PRECV_INIT</font> functions that allow partitioned  
communication setup to occur prior to message transfers. Partitioned  
communication initialization functions are local.  The partitioned  
communication initialization includes inputs on the number of  
user-visible partitions on the send-side and receive-side, which may  
differ.  Valid partitioned communication operations must have  
one or more partitions specified.  
<P> 
Once an <font face="sans-serif"> MPI_PSEND_INIT</font> call has been made, the user may start the  
operation with a call to a starting procedure and complete the  
operation with a number of <font face="sans-serif"> MPI_PREADY</font> calls equal to the  
requested number of send partitions followed by a call to a completing  
procedure. A call to <font face="sans-serif"> MPI_PREADY</font> notifies the  
<font face="sans-serif"> MPI</font> library that a specified portion of the data buffer (a specific partition) is ready to be sent.  
Notification of partial completion can be done  
via fine-grained <font face="sans-serif"> MPI_PARRIVED</font> calls at the receiver before a final  
<font face="sans-serif"> MPI_TEST</font>/<font face="sans-serif"> MPI_WAIT</font> on the request itself; the latter  
represents overall operation completion upon success.    
A full set of methods  
for starting and completing partitioned communication is given in the  
following sections.  
<P> 
 
<br> 
<em> Advice to users.</em>  
<P> 
Having a large number of receiver-side partitions can increase overheads   
as the completion mechanism may need to work with finer-grained notifications. Using a small  
number of receiver-side partitions <em> may</em> provide higher performance.  
<P> 
A large number of sender-side partitions may be aggregated by an <font face="sans-serif"> MPI</font> implementation,  
making performance concerns of a large number of sender-side partitions potentially less impactful  
than receiver-side granularity.   
 (<em> End of advice to users.</em>) <br> 
 
<br> 
<em> Advice  
        to implementors.</em>  
<P> 
It is expected that an <font face="sans-serif"> MPI</font> implementation will attempt to balance  
latency and aggregation for  
data transfers for the requested partition counts on the sender-side and receiver-side  
to allow optimization for different hardware. A high quality implementation may   
perform significant optimizations to enhance performance in this way; they may, for example,  
resize the data transfers of the partitions to combine partitions in fractional  
partition sizes (e.g., 2.5 partitions in a single data transfer).   
 (<em> End of advice to implementors.</em>) <br> 
Example <a href="node87.htm#Node87">Semantics of Partitioned Point-to-Point Communication</a> shows a simple partitioned transfer  
in which the sender-side and receiver-side partitioning is identical in partition count.  
<P> 
<br><b> Example</b>  
Simple partitioned communication example.  
<P><img width=891 height=1246 src="img32.gif" alt="Image file"><P>
  
<P> 
 
<br> 
<em> Rationale.</em>  
<P> 
Partitioned communication is designed to provide opportunities for <font face="sans-serif"> MPI</font> implementations to  
optimize data transfers. <font face="sans-serif"> MPI</font> is free to choose how many transfers to do within a partitioned communication send  
independent of how many partitions are reported as ready to <font face="sans-serif"> MPI</font> through  
<font face="sans-serif"> MPI_PREADY</font> calls.  
Aggregation of partitions is permitted but not required.  Ordering of partitions  is permitted but not required.  
A naive implementation can simply  
wait for the entire message buffer to be marked ready before any transfer(s) occur and  
could wait until the completion function is called on a request before transferring data.  
However, this modality of communication gives <font face="sans-serif"> MPI</font> implementations far more flexibility in data  
movement than  
nonpartitioned communications.  
 (<em> End of rationale.</em>) <br> 
<ul> 
</ul> 

<P>
<hr>
<a href="node86.htm#Node86"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node85.htm#Node85"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node88.htm#Node88"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node85.htm#Node85"> Partitioned Point-to-Point Communication</a>
<b>Next: </b><a href="node88.htm#Node88"> Communication Initialization and Starting with Partitioning</a>
<b>Previous: </b><a href="node86.htm#Node86"> Introduction</a>
<p>
<HR>
Return to <A HREF="node601.htm">MPI-4.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-4.1 of November 2, 2023<BR>
HTML Generated on November 19, 2023
</FONT>
</body>
</html>
